{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "722edb38-9d92-48ad-b0d4-14057e9ca2ed",
   "metadata": {},
   "source": [
    "Text classification is one of the important task in supervised machine learning (ML). \n",
    "\n",
    "It is a process of assigning tags/categories to documents helping us to automatically & quickly structure and analyze text in a cost-effective manner. \n",
    "\n",
    "It is one of the fundamental tasks in Natural Language Processing with broad applications such as sentiment-analysis, spam-detection, topic-labeling, intent-detection etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78454d76-dc7e-4155-a9ca-d605a4a9dd5d",
   "metadata": {},
   "source": [
    "Let's divide the classification problem into the below steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefdba3f-93ab-4549-83fb-e4e14512e309",
   "metadata": {},
   "source": [
    "1. Setup: Importing Libraries\n",
    "\n",
    "2. Loading the data set & Exploratory Data Analysis\n",
    "\n",
    "3. Text pre-processing\n",
    "\n",
    "4. Extracting vectors from text (Vectorization)\n",
    "\n",
    "5. Running ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e032c3b9-ee8c-4fd1-84c9-6f5150dff293",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b245298c-986c-4c5d-84ec-b86920cbec88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/local/share/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#f or word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c6f0de-2bd3-484d-9dd4-0c99a952c335",
   "metadata": {},
   "source": [
    "## 2. Loading the data set & EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c6aa40-fbea-4245-9f14-939ca882367f",
   "metadata": {},
   "source": [
    "The data set that we will be using for this article is the famous **Natural Language Processing with Disaster Tweets** (a famous Kaggle competition project: https://www.kaggle.com/c/nlp-getting-started/overview) data set where we’ll be predicting whether a given tweet is about a real disaster (target=1) or not (target=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55980d5-2a9e-49e5-8cbb-d6b566d7b840",
   "metadata": {},
   "source": [
    "In this competition, you’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which ones aren’t. \n",
    "\n",
    "You’ll have access to a dataset of 10,000 tweets that were hand classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8246a5-57a8-4d2c-ae11-c9cebca54334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
