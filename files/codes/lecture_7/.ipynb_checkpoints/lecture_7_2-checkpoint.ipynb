{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "722edb38-9d92-48ad-b0d4-14057e9ca2ed",
   "metadata": {},
   "source": [
    "Text classification is one of the important task in supervised machine learning (ML). \n",
    "\n",
    "It is a process of assigning tags/categories to documents helping us to automatically & quickly structure and analyze text in a cost-effective manner. \n",
    "\n",
    "It is one of the fundamental tasks in Natural Language Processing with broad applications such as sentiment-analysis, spam-detection, topic-labeling, intent-detection etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78454d76-dc7e-4155-a9ca-d605a4a9dd5d",
   "metadata": {},
   "source": [
    "Let's divide the classification problem into the below steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefdba3f-93ab-4549-83fb-e4e14512e309",
   "metadata": {},
   "source": [
    "1. Setup: Importing Libraries\n",
    "\n",
    "2. Loading the data set & Exploratory Data Analysis\n",
    "\n",
    "3. Text pre-processing\n",
    "\n",
    "4. Extracting vectors from text (Vectorization)\n",
    "\n",
    "5. Running ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e032c3b9-ee8c-4fd1-84c9-6f5150dff293",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b245298c-986c-4c5d-84ec-b86920cbec88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c6f0de-2bd3-484d-9dd4-0c99a952c335",
   "metadata": {},
   "source": [
    "## 2. Loading the data set & Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c6aa40-fbea-4245-9f14-939ca882367f",
   "metadata": {},
   "source": [
    "The data set that we will be using for this article is the famous **Natural Language Processing with Disaster Tweets** (a famous Kaggle competition project: https://www.kaggle.com/c/nlp-getting-started/overview) data set where we’ll be predicting whether a given tweet is about a real disaster (target=1) or not (target=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55980d5-2a9e-49e5-8cbb-d6b566d7b840",
   "metadata": {},
   "source": [
    "In this competition, you’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which ones aren’t. \n",
    "\n",
    "You’ll have access to a dataset of 10,000 tweets that were hand classified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c1ec2b-071b-41c0-9596-5be0f4a0d0bf",
   "metadata": {},
   "source": [
    "You can download the dataset from the following url:\n",
    "\n",
    "train.csv: https://raw.githubusercontent.com/zhangjianzhang/text_mining/master/files/codes/lecture_7/train.csv\n",
    "\n",
    "test.csv: https://raw.githubusercontent.com/zhangjianzhang/text_mining/master/files/codes/lecture_7/test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd59d20-3c7f-4abf-b4fb-c568b17f091c",
   "metadata": {},
   "source": [
    "After downloading the dataset, put them into the file folder containing this Jupyter file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e6cfc6-bc22-477e-a82d-e3e5446ed292",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= pd.read_csv('./train.csv')\n",
    "df_test=pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1357d26-425a-4de9-aec2-059454a881cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e1e35f6-fc0a-4b43-acc3-8ae528e3e7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0     NaN      NaN   \n",
       "1         2     NaN      NaN   \n",
       "2         3     NaN      NaN   \n",
       "3         9     NaN      NaN   \n",
       "4        11     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                   text  \n",
       "0                    Just happened a terrible car crash  \n",
       "1     Heard about #earthquake is different cities, s...  \n",
       "2     there is a forest fire at spot pond, geese are...  \n",
       "3              Apocalypse lighting. #Spokane #wildfires  \n",
       "4         Typhoon Soudelor kills 28 in China and Taiwan  \n",
       "...                                                 ...  \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n",
       "3259  Storm in RI worse than last hurricane. My city...  \n",
       "3260  Green Line derailment in Chicago http://t.co/U...  \n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n",
       "3262  #CityofCalgary has activated its Municipal Eme...  \n",
       "\n",
       "[3263 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c992b-d6ed-47d1-a47b-dedc84ed7730",
   "metadata": {},
   "source": [
    "We have 7,613 tweets in training (labelled) dataset and 3,263 in the test(unlabelled) dataset. \n",
    "\n",
    "The above is a snapshot of the training/labelled dataset which we’ll use for building our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2d5db2-68a1-477c-bbac-3a874182143a",
   "metadata": {},
   "source": [
    "### 2.1 Class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70ab547-0cf9-4bb2-bdbe-cb6b9a7a26eb",
   "metadata": {},
   "source": [
    "There are more tweets with class 0 ( no disaster) than class 1 ( disaster tweets). \n",
    "\n",
    "We can say that the dataset is relatively balanced with 4342 non-disaster tweets (57%) and 3271 disaster tweets (43%). Since the data is balanced, we won’t be applying data-balancing techniques like SMOTE while building the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b16fd323-ae8e-42d7-9718-e14b3c29d865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4342\n",
      "1    3271\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='target'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=df_train['target'].value_counts()\n",
    "print(x)\n",
    "sns.barplot(x.index,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02086ee5-e4a4-4daf-b445-eef3f75d451e",
   "metadata": {},
   "source": [
    "### 2.2 Missing values\n",
    "\n",
    "We have ~2.5k missing values in location field and 61 missing values in keyword column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cedc6773-ed41-410a-9165-ed1020e59999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08554a63-a332-4847-b383-25492eedc859",
   "metadata": {},
   "source": [
    "### 2.3 Number of words in a tweet\n",
    "\n",
    "Disaster tweets are more wordy than the non-disaster tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "496ae47d-d1e0-4102-a8d4-4c3d7807771f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.167532864567411\n",
      "14.704744357438969\n"
     ]
    }
   ],
   "source": [
    "# WORD-COUNT\n",
    "df_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\n",
    "print(df_train[df_train['target']==1]['word_count'].mean()) # Disaster tweets\n",
    "print(df_train[df_train['target']==0]['word_count'].mean()) # Non-Disaster tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4687778-3541-4802-b3a6-90c53ef0e042",
   "metadata": {},
   "source": [
    "The average number of words in a disaster tweet is **15.17** as compared to an average of **14.7** words in a non-disaster tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43d7c410-8779-4dd5-b30e-6b9f0d275904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 使用柱状图显示Disaster推文和Non-disaster推文的长度（以字符为单位）分布\n",
    "fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,4))\n",
    "train_words = df_train[df_train['target']==1]['word_count']\n",
    "ax1.hist(train_words,color='red')\n",
    "ax1.set_title('Disaster tweets')\n",
    "train_words = df_train[df_train['target']==0]['word_count']\n",
    "ax2.hist(train_words,color='green')\n",
    "ax2.set_title('Non-disaster tweets')\n",
    "fig.suptitle('Words per tweet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94329780-afe4-47ec-8a46-c7bbf156107f",
   "metadata": {},
   "source": [
    "### 2.4 Number of characters in a tweet\n",
    "\n",
    "Disaster tweets are longer than the non-disaster tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b6c7c6-7e71-40d6-8584-93a941cae173",
   "metadata": {},
   "source": [
    "The average characters in a disaster tweet is 108.1 as compared to an average of 95.7 characters in a non-disaster tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d24e78e0-9082-4346-b902-5359efc7b296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108.11342097217977\n",
      "95.70681713496084\n"
     ]
    }
   ],
   "source": [
    "# CHARACTER-COUNT\n",
    "df_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\n",
    "print(df_train[df_train['target']==1]['char_count'].mean()) #Disaster tweets\n",
    "print(df_train[df_train['target']==0]['char_count'].mean()) #Non-Disaster tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb922775-2321-4dc2-a743-adf0c3c27a69",
   "metadata": {},
   "source": [
    "## 3. Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8011d96e-6d2c-44cf-9450-913611804d3d",
   "metadata": {},
   "source": [
    "Before we move to model building, we need to preprocess our dataset by removing punctuations & special characters, cleaning texts, removing stop words, and applying lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa3cdb7-58c0-401d-967f-746e1d8063d3",
   "metadata": {},
   "source": [
    "Simple text cleaning processes: Some of the common text cleaning process involves:\n",
    "\n",
    "- Removing punctuations, special characters, URLs & hashtags\n",
    "\n",
    "- Removing leading, trailing & extra white spaces/tabs\n",
    "\n",
    "- Typos, slangs are corrected, abbreviations are written in their long forms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376432c7-b681-4796-9e95-388b82d91260",
   "metadata": {},
   "source": [
    "**Stop-word removal**: We can remove a list of generic stop words from the English vocabulary using nltk. A few such words are ‘i’,’you’,’a’,’the’,’he’,’which’ etc.\n",
    "\n",
    "**Stemming**: Refers to the process of slicing the end or the beginning of words with the intention of removing affixes(prefix/suffix)\n",
    "\n",
    "**Lemmatization**: It is the process of reducing the word to its base form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d5935d-2ea3-4e77-a833-9d6060812c50",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "<img src=\"https://github.com/zhangjianzhang/text_mining/blob/master/files/codes/lecture_7/stem_lemma.png?raw=true\">\n",
    "<br>\n",
    "<center><em><strong>Stemming V.S. Lemmatization</strong></em></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fce5c778-3ad4-4250-aaec-516364df7570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lowercase, strip and remove punctuations\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text=text.strip()\n",
    "    # 去掉 <word>\n",
    "    text=re.compile('<.*?>').sub('', text)\n",
    "    # 标点替换为空格\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    # 空白字符替换为空格\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    # [digit] 替换为空格\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "    # 去掉非单词字符和空白字符\n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    # 数字替换为空格\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    # 多个空白替换为空格 \n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c55c5032-de2c-46b6-910d-5ce6d25533ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6cdc847-7ef2-4eed-9fe1-d77724a4eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "# 指定词性会让词形还原更准确\n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb44a9b-7832-41ca-9941-855b390dc7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7578229-f9d9-46e6-bff5-0c8cca9c71d2",
   "metadata": {},
   "source": [
    "**Final pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9b1ead8-51d5-4d09-96cc-1bcc9ef7c37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>69</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>38</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>133</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>65</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>88</td>\n",
       "      <td>get sent photo ruby alaska smoke wildfires pou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  word_count  char_count  \\\n",
       "0       1          13          69   \n",
       "1       1           7          38   \n",
       "2       1          22         133   \n",
       "3       1           8          65   \n",
       "4       1          16          88   \n",
       "\n",
       "                                          clean_text  \n",
       "0         deed reason earthquake may allah forgive u  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  resident ask shelter place notify officer evac...  \n",
       "3  people receive wildfire evacuation order calif...  \n",
       "4  get sent photo ruby alaska smoke wildfires pou...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "df_train['clean_text'] = df_train['text'].apply(lambda x: finalpreprocess(x))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f51139-6dfb-479f-83dd-ace1b1caa806",
   "metadata": {},
   "source": [
    "## 4. Extracting vectors from text (Vectorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4121ff7e-13fd-48f8-a7f2-f4f2baac8e86",
   "metadata": {},
   "source": [
    "It’s difficult to work with text data while building Machine learning models since these models need well-defined numerical data. \n",
    "\n",
    "**The process to convert text data into numerical data/vector, is called vectorization or in the NLP world, word embedding.**\n",
    "\n",
    "**Bag-of-Words(BoW)** and **Word Embedding (with Word2Vec)** are two well-known methods for **converting text data to numerical data**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e1723-c27b-449f-944e-8e3be72a08cd",
   "metadata": {},
   "source": [
    "**There are a few versions of Bag of Words, corresponding to different words scoring methods.**\n",
    "\n",
    "We use the `scikit-learn` library to calculate the BoW numerical values using these approaches:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97023b86-8e05-4890-b155-40ac43696d56",
   "metadata": {},
   "source": [
    "### 4.1 Count vectors\n",
    "\n",
    "It builds a vocabulary from a corpus of documents and counts how many times the words appear in each document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a6e7d1-073a-4e5e-9302-ef4685aeeaac",
   "metadata": {},
   "source": [
    "### 4.2 Term Frequency-Inverse Document Frequencies (tf-Idf)\n",
    "\n",
    "Count vectors might not be the best representation for converting text data to numerical data. \n",
    "\n",
    "So, instead of simple counting, we can also use an advanced variant of the Bag-of-Words that uses the term frequency–inverse document frequency (or Tf-Idf). \n",
    "\n",
    "**Basically, the value of a word increases proportionally to count in the document, but it is inversely proportional to the frequency of the word in the corpus.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c8cfd9-9570-4dd1-bb8c-45dfbd75f8f9",
   "metadata": {},
   "source": [
    "### 4.3 Word2Vec\n",
    "\n",
    "One of the major drawbacks of using Bag-of-words techniques is that **it can't capture the meaning or relation of the words from vectors**. \n",
    "\n",
    "Word2Vec is one of the most popular technique to learn **word embeddings** (词向量) using shallow neural network which is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fe2496-4f0b-43b7-8e3d-ae1af9d07075",
   "metadata": {},
   "source": [
    "We can use any of these approaches to convert our text data to numerical form which will be used to build the classification model. \n",
    "\n",
    "With this in mind, we are going to first partition the dataset into training set (80%) and test set (20%) using the below-mentioned code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3822569a-3b87-47f1-95da-7034fec3d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAIN AND TEST\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[\"clean_text\"],df_train[\"target\"],test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72d7867d-0d27-4420-9448-321e6b25186c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572     miss gary busey son play dixie electronic gree...\n",
       "5161    gop debate tonight jon stewart next week oblit...\n",
       "4538                          chikislizeth injure anymore\n",
       "5652    ud rescue structural collapse scott road ypres...\n",
       "7000    happy twister thank laugh stick side matter al...\n",
       "                              ...                        \n",
       "6019    seismic risk comparison case study calabria ma...\n",
       "223     one thing sure god promise israel annihilate h...\n",
       "3591    year old boy charge manslaughter toddler repor...\n",
       "4349    rickybonessxm fuck u specially new shit first ...\n",
       "1849    wce even lie even tho stand still always crush...\n",
       "Name: clean_text, Length: 6090, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc212fbc-f884-4696-bce8-ed01bb1a83e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572     0\n",
       "5161    0\n",
       "4538    0\n",
       "5652    1\n",
       "7000    0\n",
       "       ..\n",
       "6019    1\n",
       "223     0\n",
       "3591    1\n",
       "4349    0\n",
       "1849    0\n",
       "Name: target, Length: 6090, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d132a31b-f889-4878-9942-ecd8ee0d88ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec runs on tokenized sentences\n",
    "X_train_tok = [nltk.word_tokenize(i) for i in X_train]  \n",
    "X_test_tok = [nltk.word_tokenize(i) for i in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03173066-9817-46e8-aace-4b0a79a3df2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['miss', 'gary', 'busey', 'son', 'play', 'dixie', 'electronic', 'green', 'fiddle', 'post', 'battle', 'celebration', 'sequence'], ['gop', 'debate', 'tonight', 'jon', 'stewart', 'next', 'week', 'obliterate', 'jonvoyage']]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tok[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79996c34-3c88-4557-93ad-65937e9346b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf-Idf\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aea43e6e-7625-4a0e-ae14-1e50b9392d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building Word2Vec model\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "    def fit(self, X, y):\n",
    "            return self\n",
    "    \n",
    "    # 将文本中所有词的向量求平均表示句子向量\n",
    "    def transform(self, X):\n",
    "            return np.array([\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                        or [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db24566f-5ac3-4ef9-9fb2-554d5edf9911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load other pretrained word2vector model, e.g., GoogleNews-vectors-negative300.bin.gz released by Google\n",
    "\n",
    "# model = Word2Vec.load_word2vec_format('/tmp/vectors.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1babd400-c62b-430d-b58e-9474b5d05c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vector model with our training data\n",
    "model = Word2Vec(X_train_tok + X_test_tok, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d124202-192d-40dd-8384-e55c36fa7e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))\n",
    "modelw = MeanEmbeddingVectorizer(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6859d10e-ebd9-4408-ba80-80bfbf30a62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting text to numerical data using Word2Vec\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_test_vectors_w2v = modelw.transform(X_test_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32fe30d-6f3a-45ab-9743-144a61f31fd6",
   "metadata": {},
   "source": [
    "### 5. Running ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac505a6-3daa-4c78-8a42-9cc1916274bc",
   "metadata": {},
   "source": [
    "It's time to train a machine learning model on the vectorized dataset and test it. \n",
    "\n",
    "Now that we have converted the text data to numerical data, we can run ML models on **X_train_vector_tfidf** & **y_train**.\n",
    "\n",
    "We'll test this model on **X_test_vectors_tfidf** to get **y_predict** and further evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f82cc8b-c5eb-4e19-a03c-b0592279d99d",
   "metadata": {},
   "source": [
    "**Logistic Regression**\n",
    "\n",
    "We will start with the most simplest one Logistic Regression. You can easily build a LogisticRegression in scikit using below lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fb0462f-c0e6-4fb7-8387-49b9c3169d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.82       854\n",
      "           1       0.77      0.74      0.76       669\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.79      0.79      0.79      1523\n",
      "weighted avg       0.79      0.79      0.79      1523\n",
      "\n",
      "Confusion Matrix: [[710 144]\n",
      " [174 495]]\n",
      "AUC: 0.853407686679759\n"
     ]
    }
   ],
   "source": [
    "# FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
    "lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)\n",
    "\n",
    "# Predict y value for test dataset\n",
    "y_predict = lr_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65046d21-ec74-40a3-9c6a-7ed48e281994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.83      0.72       854\n",
      "           1       0.65      0.41      0.51       669\n",
      "\n",
      "    accuracy                           0.64      1523\n",
      "   macro avg       0.65      0.62      0.61      1523\n",
      "weighted avg       0.65      0.64      0.63      1523\n",
      "\n",
      "Confusion Matrix: [[706 148]\n",
      " [393 276]]\n",
      "AUC: 0.7037794534118874\n"
     ]
    }
   ],
   "source": [
    "# FITTING THE CLASSIFICATION MODEL using Logistic Regression (W2v)\n",
    "lr_w2v=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "lr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n",
    "\n",
    "# Predict y value for test dataset\n",
    "y_predict = lr_w2v.predict(X_test_vectors_w2v)\n",
    "y_prob = lr_w2v.predict_proba(X_test_vectors_w2v)[:,1]\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb824bf-6a9e-45f6-b6f3-2398278e6e53",
   "metadata": {},
   "source": [
    "**Naive Bayes**\n",
    "\n",
    "It's a probabilistic classifier that makes use of Bayes’ Theorem, a rule that uses probability to make predictions based on prior knowledge of conditions that might be related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9de1dd9a-c0cb-4d42-a8e1-b9809dfb61fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.92      0.84       854\n",
      "           1       0.86      0.65      0.74       669\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.81      0.78      0.79      1523\n",
      "weighted avg       0.81      0.80      0.79      1523\n",
      "\n",
      "Confusion Matrix: [[783  71]\n",
      " [234 435]]\n",
      "AUC: 0.8548044373965127\n"
     ]
    }
   ],
   "source": [
    "# FITTING THE CLASSIFICATION MODEL using Naive Bayes(tf-idf)\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(X_train_vectors_tfidf, y_train)  \n",
    "\n",
    "# Predict y value for test dataset\n",
    "y_predict = nb_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = nb_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2087f27-7097-49dd-9243-cc798bd326ef",
   "metadata": {},
   "source": [
    "After selecting the best model, you can  to estimate `target` values for the unlabelled dataset (`df_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a0de45e-0962-4ae9-ba82-015ed9a51825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          clean_text  target\n",
      "0                          happen terrible car crash       1\n",
      "1  heard earthquake different city stay safe ever...       1\n",
      "2  forest fire spot pond geese flee across street...       1\n",
      "3                  apocalypse light spokane wildfire       1\n",
      "4                 typhoon soudelor kill china taiwan       1\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing the new dataset\n",
    "df_test['clean_text'] = df_test['text'].apply(lambda x: finalpreprocess(x)) #preprocess the data\n",
    "X_test=df_test['clean_text'] \n",
    "\n",
    "# converting words to numerical data using tf-idf\n",
    "X_vector=tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# use logistic regression with TF-IDF to predict 'target' value for the new dataset \n",
    "y_predict = lr_tfidf.predict(X_vector)      \n",
    "y_prob = lr_tfidf.predict_proba(X_vector)[:,1]\n",
    "\n",
    "# organize the prediction results\n",
    "df_test['predict_prob'] = y_prob\n",
    "df_test['target'] = y_predict\n",
    "final = df_test[['clean_text','target']].reset_index(drop=True)\n",
    "print(final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6605dd1-71c8-42e7-864d-1e6f0df67424",
   "metadata": {},
   "source": [
    "In this article, we demonstrated the basics of building a text classification model comparing Bag-of-Words (with Tf-Idf) and Word Embedding (with Word2Vec). \n",
    "\n",
    "You can further enhance the performance of your model using this code by:\n",
    "\n",
    "- using other classification algorithms like Support Vector Machines (SVM), XgBoost, Ensemble models, Neural networks etc.\n",
    "- using Gridsearch to tune the hyperparameters of your model\n",
    "- using advanced word-embedding methods like GloVe and BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c3a0cd-082d-4a24-b2d8-f7e2e3ce7166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
